Inference
	
Parameter
	
Statistic
	
Type of Data
	
Examples
	
Analysis
	
Minitab Command
	
Conditions
1 	Estimating a Mean 	

One Population Mean

\(\mu\)
	

Sample Mean

\(\bar{x}\)
	Numerical 	

What is the average weight of adults?

What is the average cholesterol level of adult females?
	

1-sample t-interval

\(\bar{x}\pm t_{\alpha /2}\cdot \frac{s}{\sqrt{n}}\)
	Stat > Basic statistics > 1-sample t 	

data approximately normal

OR

have a large sample size (n ≥ 30)
2 	Test About a Mean 	

One population Mean

\(\mu\)
	

Sample Mean

\(\bar{x}\)
	Numerical 	

Is the average GPA of juniors at Penn State higher than 3.0?

Is the average winter temperature in State College less than 42°F?
	

\(H_0: \mu = \mu_0\)

\(H_a: \mu \ne \mu_0\)
OR
\(H_a: \mu > \mu_0\)
OR
\(H_a: \mu < \mu_0\)

The 1-sample t-test:

\(t=\frac{\bar{x}-\mu_{0}}{\frac{s}{\sqrt{n}}}\)
	Stat > Basic statistics > 1-sample t 	

data approximately normal

OR

have a large sample size (n ≥ 30)
3 	Estimating a Proportion 	

One Population Proportion

\(p\)
	

Sample Proportion

\(\hat{p}\)
	Categorical (Binary) 	

What is the proportion of males in the world?

What is the proportion of students that smoke?
	

1-proportion Z-interval

\(\hat{p}\pm z_{\alpha /2}\sqrt{\frac{\hat{p}\cdot \left ( 1-\hat{p} \right )}{n}}\)
	Stat > Basic statistics > 1-sample proportion 	

have at least 5 in each category
4 	Test About a Proportion 	

One Population Proportion

\(p\)
	Sample Proportion \(\hat{p}\) 	Categorical (Binary) 	

Is the proportion of females different from 0.5?

Is the proportion of students who fail STAT 500 less than 0.1?
	

\(H_0: p = p_0\)

\(H_a: p \ne p_0\)
OR
\(H_a: p > p_0\)
OR
\(H_a: p < p_0\)

The one proportion Z-test:

\(z=\frac{\hat{p}-p _{0}}{\sqrt{\frac{p _{0}\left ( 1- p _{0}\right )}{n}}}\)
	Stat > Basic statistics > 1-sample proportion 	

\(np_0 \geq 5\) and \(n (1 - p_0) \geq 5\)
5 	Estimating the Difference of Two Means 	

Difference in two population means

\(\mu_1 - \mu_2\)
	

Difference in two sample means

\(\bar{x}_{1} - \bar{x}_{2}\)
	Numerical 	

How different are the mean GPAs of males and females?

How many fewer colds do vitamin C takers get, on average, than non-vitamin takers?
	

2-sample t-interval

\(\bar{x}_{1}-\bar{x}_{2}\pm t_{\alpha /2}\cdot s.e.\left (\bar{x}_{1}-\bar{x}_{2}  \right )\)
	Stat > Basic statistics > 2-sample t 	

Independent samples from the two populations

Data in each sample are about normal or large samples
6 	Test to Compare Two Means 	

Difference in two population means

\(\mu_1 - \mu_2\)
	

Difference in two sample means

\(\bar{x}_{1} - \bar{x}_{2}\)
	Numerical 	

Do the mean pulse rates of exercisers and non-exercisers differ?

Is the mean EDS score for dropouts greater than the mean EDS score for graduates?
	

\(H_0: \mu_1 = \mu_2\)

\(H_a: \mu_1 \ne \mu_2\) OR \(H_a: \mu_1 > \mu_2\) OR \(H_a: \mu_1 < \mu_1\)

The 2-sample t-test:

\(t=\frac{\left (\bar{x}_{1}-\bar{x}_{2}  \right )-0}{s.e.\left (\bar{x}_{1}-\bar{x}_{2}  \right )} \)
	Stat > Basic statistics > 2-sample t 	

Independent samples from the two populations

Data in each sample are about normal or large samples
7 	Estimating a Mean with Paired Data 	

Mean of paired difference

\(\mu_D\)
	

Sample mean of difference

\(\bar{d}\)
	Numerical 	

What is the difference in pulse rates, on the average, before and after exercise?
	

paired t-interval

\(\bar{d}\pm t_{\alpha /2}\cdot \frac{s_{d}}{\sqrt{n}}\)
	Stat > Basic statistics > Paired t 	

Differences approximately normal

OR

Have a large number of pairs (n ≥ 30)
8 	Test About a Mean with Paired Data 	

Mean of paired difference

\(\mu_D\)
	

Sample mean of difference

\(\bar{d}\)
	Numerical 	

Is the difference in IQ of pairs of twins zero?

Are the pulse rates of people higher after exercise?
	

\(H_0: \mu_D = 0\)

\(H_a: \mu_D \ne 0\)
OR
\(H_a: \mu_D > 0\)
OR
\(H_a: \mu_D < 0\)

\(t=\frac{\bar{d}-0}{\frac{s_{d}}{\sqrt{n}}}\)
	Stat > Basic statistics > Paired t 	

Differences approximately normal

OR

Have a large number of pairs (n ≥ 30)
9 	Estimating the Difference of Two Proportions 	

Difference in two population proportions

\(p_1 - p_2\)
	

Difference in two sample proportions

\(\hat{p}_{1} - \hat{p}_{2}\)
	Categorical (Binary) 	

How different are the percentages of male and female smokers?

How different are the percentages of upper- and lower-class binge drinkers?
	

two-proportions Z-interval

\(\hat{p _{1}}-\hat{p _{2}}\pm z_{\alpha /2}\cdot s.e.\left ( \hat{p _{1}}-\hat{p _{2}} \right )\)
	Stat > Basic statistics > 2 proportions 	

Independent samples from the two populations

Have at least 5 in each category for both populations
10 	Test to Compare Two Proportions 	

Difference in two population proportions

\(p_1 - p_2\)
	

Difference in two sample proportions

\(\hat{p}_{1} - \hat{p}_{2}\)
	Categorical (Binary) 	

Is the percentage of males with lung cancer higher than the percentage of females with lung cancer?

Are the percentages of upper- and lower- class binge drinkers different?
	

\(H_0: p_1 = p_2\)

\(H_a: p_1 \ne p_2 \)
OR
\(H_a: p_1 > p_2\) OR
\(H_a: p_1 < p_2\)

The two proportion z test:

\(z=\frac{\hat{p}_{1}-\hat{p}_{2}}{\sqrt{\hat{p}\left ( 1-\hat{p} \right )\left ( \frac{1}{n_{1}}+ \frac{1}{n_{2}}\right )}}\)

\(\hat{p}=\frac{x_{1}+x_{2}}{n_{1}+n_{2}}\)
	Stat > Basic statistics > 2 proportions 	

Independent samples from the two populations

Have at least 5 in each category for both populations
11 	Relationship in a 2-Way Table 	Relationship between two categorical variables or difference in two or more population proportions 	The observed counts in a two-way table 	Categorical 	

Is there a relationship between smoking and lung cancer?

Do the proportions of students in each class who smoke differ?
	

Ho: The two variables are not related

Ha: The two variables are related

The chi-square statistic:

\(X^2=\sum_{\text{all cells}}\frac{(\text{Observed-Expected})^2}{\text{Expected}}\)
	Stat > Tables > Chi square Test 	

All expected counts should be greater than 1

At least 80% of the cells should have an expected count greater than 5
12 	Test About a Slope 	

Slope of the population regression line

\(\beta_1\)
	

Sample estimate of the slope

b1
	Numerical 	

Is there a linear relationship between height and weight of a person?
	

\(H_0: \beta_1 = 0\)

\(H_a: \beta_1 \ne 0\) OR \(H_a: \beta_1 > 0\) OR \(H_a: \beta_1 < 0\)

The t-test with n - 2 degrees of freedom:

\(t=\frac{b_{1}-0}{s.e.\left ( b_{1} \right )}\)
	Stat > Regression > Regression 	

The form of the equation that links the two variables must be correct

The error terms are normally distributed

The errors terms have equal variances

The error terms are independent of each other
13 	Test to Compare Several Means 	

Population means of the t populations

\(\mu_1, \mu_2, \cdots , \mu_t\)
	

Sample means of the t populations

\(x_1, x_2, \cdots , x_t\)
	Numerical 	

Is there a difference between the mean GPA of freshman, sophomore, junior, and senior classes?
	

\(H_0: \mu_1 = \mu_2 = ... = \mu_t\)

\(H_a: \text{not all the means are equal}\)

The F-test for one-way ANOVA:

\(F=\frac{MSTR}{MSE}\)
	Stat > ANOVA > Oneway 	

Each population is normally distributed

Independent samples from the t populations

Equal population standard deviations
14 	Test of Strength & Direction of Linear Relationship of 2 Quantitative Variables 	

Population Correlation

\(\rho\)

"rho"
	

Sample correlation

\(r\)
	Numerical 	Is there a linear relationship between height and weight? 	

\(H_0: \rho = 0\)

\(H_a: \rho \ne 0\)

\(t=\frac{r\sqrt{n-2}}{\sqrt{1-r^2}}\)
	Stat > Basic Statistics > Correlation 	

2 variables are continuous

Related pairs

No significant outliers

Normality of both variables

Linear relationship between the variables
15 	Test to Compare Two Population Variances 	

Population variances of 2 populations

\(\sigma_{1}^{2}, \sigma_{2}^{2}\)
	

Sample variances of 2 populations

\(s_{1}^{2}, s_{2}^{2}\)
	Numerical 	

Are the variances of length of lumber produced by Company A different from those produced by Company B
	

\(H_0: \sigma_{1}^{2} = \sigma_{2}^{2}\)

\(H_2:  \sigma_{1}^{2} \ne \sigma_{2}^{2}\)

\(F=\frac{s_{1}^{2}}{s_{2}^{2}}\)
	Stat > Basic statistics > 2 variances 	

Each population is normally distributed

Independent samples from the 2 populations


Statistics is the study of the collection, analysis, interpretation, presentation, and organization of data.[1] In statistics to, e.g., a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model process to be studied. Populations can be diverse topics such as "all people living in a country" or "every atom composing a crystal". Statistics deals with all aspects of data including the planning of data collection in terms of the design of surveys and experiments.[1]

Some popular definitions are:

    Merriam-Webster dictionary defines statistics as "classified facts representing the conditions of a people in a state – especially the facts that can be stated in numbers or any other tabular or classified arrangement[2]".
    Statistician Sir Arthur Lyon Bowley defines statistics as "Numerical statements of facts in any department of inquiry placed in relation to each other[3]".

When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not experimental manipulation.

Two main statistical methodologies are used in data analysis: descriptive statistics, which summarizes data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draws conclusions from data that are subject to random variation (e.g., observational errors, sampling variation).[4] Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.

A standard statistical procedure involves the test of the relationship between two statistical data sets, or a data set and a synthetic data drawn from idealized model. An hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a "false positive") and Type II errors (null hypothesis fails to be rejected and an actual difference between populations is missed giving a "false negative").[5] Multiple problems have come to be associated with this framework: ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.[citation needed]

Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems.

Statistics can be said to have begun in ancient civilization, going back at least to the 5th century BC, but it was not until the 18th century that it started to draw more heavily from calculus and probability theory. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.
Contents

    1 Scope
        1.1 Mathematical statistics
    2 Overview
    3 Data collection
        3.1 Sampling
        3.2 Experimental and observational studies
    4 Types of data
    5 Terminology and theory of inferential statistics
        5.1 Statistics, estimators and pivotal quantities
        5.2 Null hypothesis and alternative hypothesis
        5.3 Error
        5.4 Interval estimation
        5.5 Significance
        5.6 Examples
    6 Misuse of statistics
        6.1 Misinterpretation: correlation
    7 History of statistical science
    8 Applications
        8.1 Applied statistics, theoretical statistics and mathematical statistics
        8.2 Machine learning and data mining
        8.3 Statistics in society
        8.4 Statistical computing
        8.5 Statistics applied to mathematics or the arts
    9 Specialized disciplines
    10 See also
    11 References
    12 Further reading
    13 External links

Scope

Statistics is a mathematical body of science that pertains to the collection, analysis, interpretation or explanation, and presentation of data,[6] or as a branch of mathematics.[7] Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is concerned with the use of data in the context of uncertainty and decision making in the face of uncertainty.[8][9]
Mathematical statistics
Main article: Mathematical statistics

Mathematical statistics is the application of mathematics to statistics, which was originally conceived as the science of the state — the collection and analysis of facts about a country: its economy, land, military, population, and so forth. Mathematical techniques used for this  mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory.[10][11]
Overview

In statistics to a problem, it is common practice to start with a population or process to be studied. Populations can be diverse topics such as "all persons living in a country" or "every atom composing a crystal".

Ideally, statisticians compile data about the entire population (an operation called census). This may be organized by governmental statistical institutes. Descriptive statistics can be used to summarize the population data. Numerical descriptors  mean and standard deviation for continuous data types (like income), while frequency and percentage are more useful in terms of describing categorical data (like race).

When a census is not feasible, a chosen subset of the population called a sample is studied. Once a sample that is representative of the population is determined, data is collected for the sample members in an observational or experimental setting. Again, descriptive statistics can be used to summarize the sample data. However, the drawing of the sample has been subject to an element of randomness, hence the established numerical descriptors from the sample are also due to uncertainty. To still draw meaningful conclusions about the entire population, inferential statistics is needed. It uses patterns in the sample data to draw inferences about the population represented, accounting for randomness. These inferences may take the form of: answering yes/no questions about the data (hypothesis testing), estimating numerical characteristics of the data (estimation), describing associations within the data (correlation) and modeling relationships within the data (for example, using regression analysis). Inference can extend to forecasting, prediction and estimation of unobserved values either in or associated with the population being studied; it can  extrapolation and interpolation of time series or spatial data, and can also  data mining.
Data collection
Sampling

When full census data cannot be collected, statisticians collect sample data by developing specific experiment designs and survey samples. Statistics itself also provides tools for prediction and forecasting the use of data through statistical models. To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design for experiments that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.

Sampling theory is part of the mathematical discipline of probability theory. Probability is used in mathematical statistics to study the sampling distributions of sample statistics and, more generally, the properties of statistical procedures. The use of any statistical method is valid when the system or population under consideration satisfies the assumptions of the method. The difference in point of view between classic probability theory and sampling theory is, roughly, that probability theory starts from the given parameters of a total population to deduce probabilities that pertain to samples. Statistical inference, however, moves in the opposite direction—inductively inferring from samples to the parameters of a larger or total population.
Experimental and observational studies

A common goal for a statistical research project is to investigate causality, and in particular to draw a conclusion on the effect of changes in the values of predictors or independent variables on dependent variables. There are two major types of causal statistical studies: experimental studies and observational studies. In both types of studies, the effect of differences of an independent variable (or variables) on the behavior of the dependent variable are observed. The difference between the two types lies in how the study is actually conducted. Each can be very effective. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not experimental manipulation. Instead, data are gathered and correlations between predictors and response are investigated. While the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data – like natural experiments and observational studies[12] – for which a statistician would use a modified, more structured estimation method (e.g., Difference in differences estimation and instrumental variables, among many others) that produce consistent estimators.
Experiments

The basic steps of a statistical experiment are:

    Planning the research, including finding the number of replicates of the study, using the following information: preliminary estimates regarding the size of treatment effects, alternative hypotheses, and the estimated experimental variability. Consideration of the selection of experimental subjects and the ethics of research is necessary. Statisticians recommend that experiments compare (at least) one new treatment with a standard treatment or control, to allow an unbiased estimate of the difference in treatment effects.
    Design of experiments, using blocking to reduce the influence of confounding variables, and randomized assignment of treatments to subjects to allow unbiased estimates of treatment effects and experimental error. At this stage, the experimenters and statisticians write the experimental protocol that will guide the performance of the experiment and which specifies the primary analysis of the experimental data.
    Performing the experiment following the experimental protocol and analyzing the data following the experimental protocol.
    Further examining the data set in secondary analyses, to suggest new hypotheses for future study.
    Documenting and presenting the results of the study.

Experiments on human behavior have special concerns. The famous Hawthorne study examined changes to the working environment at the Hawthorne plant of the Western Electric Company. The researchers were interested in determining whether increased illumination would increase the productivity of the assembly line workers. The researchers first measured the productivity in the plant, then modified the illumination in an area of the plant and checked if the changes in illumination affected productivity. It turned out that productivity indeed improved (under the experimental conditions). However, the study is heavily criticized today for errors in experimental procedures, specifically for the lack of a control group and blindness. The Hawthorne effect refers to finding that an outcome (in this case, worker productivity) changed due to observation itself. Those in the Hawthorne study became more productive not because the lighting was changed but because they were being observed.[13]
Observational study

An example of an observational study is one that explores the association between smoking and lung cancer. This type of study typically uses a survey to collect observations about the area of interest and then performs statistical analysis. In this case, the researchers would collect observations of both smokers and non-smokers, perhaps through a case-control study, and then look for the number of cases of lung cancer in each group.
Types of data
Main articles: Statistical data type and Levels of measurement

Various attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in Celsius or Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.

Because variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented.

Other categorizations have been proposed. For example, Mosteller and Tukey (1977)[14] distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990)[15] described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998),[16] van den Berg (1991).[17]

The issue of whether or not it is appropriate to different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. "The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer" (Hand, 2004, p. 82).[18]
Terminology and theory of inferential statistics
Statistics, estimators and pivotal quantities

Consider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables.[19] The population being examined is described by a probability distribution that may have unknown parameters.

A statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters.

Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators  sample mean, unbiased sample variance and sample covariance.

A random variable that is a function of the random sample and of the unknown parameter, but whose probability distribution does not depend on the unknown parameter is called a pivotal quantity or pivot. Widely used pivots  the z-score, the chi square statistic and Student's t-value.

Between two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.

Other desirable properties for estimators : UMVUE estimators that have the lowest variance for all possible values of the parameter to be estimated (this is usually an easier property to verify than efficiency) and consistent estimators which converges in probability to the true value of such parameter.

This still leaves the question of how to obtain estimators in a given situation and carry the computation, several methods have been proposed: the method of moments, the maximum likelihood method, the least squares method and the more recent method of estimating equations.
Null hypothesis and alternative hypothesis

Interpretation of statistical information can often the development of a null hypothesis which is usually (but not necessarily) that no relationship exists among variables or that no change occurred over time.[20][21]

The best illustration for a novice is the predicament encountered by a criminal trial. The null hypothesis, H0, asserts that the defendant is innocent, whereas the alternative hypothesis, H1, asserts that the defendant is guilty. The indictment comes because of suspicion of the guilt. The H0 (status quo) stands in opposition to H1 and is maintained unless H1 is supported by evidence "beyond a reasonable doubt". However, "failure to reject H0" in this case does not imply innocence, but merely that the evidence was insufficient to convict. So the jury does not necessarily accept H0 but fails to reject H0. While one can not "prove" a null hypothesis, one can test how close it is to being true with a power test, which tests for type II errors.

What statisticians call an alternative hypothesis is simply an hypothesis that contradicts the null hypothesis.
Error

Working from a null hypothesis, two basic forms of error are recognized:

    Type I errors where the null hypothesis is falsely rejected giving a "false positive".
    Type II errors where the null hypothesis fails to be rejected and an actual difference between populations is missed giving a "false negative".

Standard deviation refers to the extent to which individual observations in a sample differ from a central value, such as the sample or population mean, while Standard error refers to an estimate of difference between sample mean and population mean.

A statistical error is the amount by which an observation differs from its expected value, a residual is the amount an observation differs from the value the estimator of the expected value assumes on a given sample (also called prediction).

Mean squared error is used for obtaining efficient estimators, a widely used class of estimators. Root mean square error is simply the square root of mean squared error.
A least squares fit: in red the points to be fitted, in blue the fitted line.

Many statistical methods seek to minimize the residual sum of squares, and these are called "methods of least squares" in contrast to Least absolute deviations. The latter gives equal weight to small and big errors, while the former gives more weight to large errors. Residual sum of squares is also differentiable, which provides a handy property for doing regression. Least squares applied to linear regression is called ordinary least squares method and least squares applied to nonlinear regression is called non-linear least squares. Also in a linear regression model the non deterministic part of the model is called error term, disturbance or more simply noise. Both linear regression and non-linear regression are addressed in polynomial least squares, which also describes the variance in a prediction of the dependent variable (y axis) as a function of the independent variable (x axis) and the deviations (errors, noise, disturbances) from the estimated (fitted) curve.

Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data and/or censoring may result in biased estimates and specific techniques have been developed to address these problems.[22]
Interval estimation
Main article: Interval estimation
Confidence intervals: the red line is true value for the mean in this example, the blue lines are random confidence intervals for 100 realizations.

Most studies only sample part of a population, so results don't fully represent the whole population. Any estimates obtained from the sample only approximate the population value. Confidence intervals allow statisticians to express how closely the sample estimate matches the true value in the whole population. Often they are expressed as 95% confidence intervals. Formally, a 95% confidence interval for a value is a range where, if the sampling and analysis were repeated under the same conditions (yielding a different dataset), the interval would  the true (population) value in 95% of all possible cases. This does not imply that the probability that the true value is in the confidence interval is 95%. From the frequentist perspective, such a claim does not even make sense, as the true value is not a random variable. Either the true value is or is not within the given interval. However, it is true that, before any data are sampled and given a plan for how to construct the confidence interval, the probability is 95% that the yet-to-be-calculated interval will cover the true value: at this point, the limits of the interval are yet-to-be-observed random variables. One approach that does yield an interval that can be interpreted as having a given probability of containing the true value is to use a credible interval from Bayesian statistics: this approach depends on a different way of interpreting what is meant by "probability", that is as a Bayesian probability.

In principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.
Significance
Main article: Statistical significance

Statistics rarely give a simple Yes/No type answer to the question under analysis. Interpretation often comes down to the level of statistical significance applied to the numbers and often refers to the probability of a value accurately rejecting the null hypothesis (sometimes referred to as the p-value).
In this graph the black line is probability distribution for the test statistic, the critical region is the set of values to the right of the observed data point (observed value of the test statistic) and the p-value is represented by the green area.

The standard approach[19] is to test a null hypothesis against an alternative hypothesis. A critical region is the set of values of the estimator that leads to refuting the null hypothesis. The probability of type I error is therefore the probability that the estimator belongs to the critical region given that null hypothesis is true (statistical significance) and the probability of type II error is the probability that the estimator doesn't belong to the critical region given that the alternative hypothesis is true. The statistical power of a test is the probability that it correctly rejects the null hypothesis when the null hypothesis is false.

Referring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.

While in principle the acceptable level of statistical significance may be subject to debate, the p-value is the smallest significance level that allows the test to reject the null hypothesis. This is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the p-value, the lower the probability of committing type I error.

Some problems are usually associated with this framework (See criticism of hypothesis testing):

    A difference that is highly statistically significant can still be of no practical significance, but it is possible to properly formulate tests to account for this. One response involves going beyond reporting only the significance level to  the p-value when reporting whether a hypothesis is rejected or accepted. The p-value, however, does not indicate the size or importance of the observed effect and can also seem to exaggerate the importance of minor differences in large studies. A better and increasingly common approach is to confidence intervals. Although these are produced from the same calculations as those of hypothesis tests or p-values, they describe both the size of the effect and the uncertainty surrounding it.
    Fallacy of the transposed conditional, aka prosecutor's fallacy: criticisms arise because the hypothesis testing approach forces one hypothesis (the null hypothesis) to be favored, since what is being evaluated is probability of the observed result given the null hypothesis and not probability of the null hypothesis given the observed result. An alternative to this approach is offered by Bayesian inference, although it requires establishing a prior probability.[23]
    Rejecting the null hypothesis does not automatically prove the alternative hypothesis.
    As everything in inferential statistics it relies on sample size, and therefore under fat tails p-values may be seriously mis-computed.

Examples

Some well-known statistical tests and procedures are:

    Analysis of variance (ANOVA)
    Chi-squared test
    Correlation
    Factor analysis
    Mann–Whitney U
    Mean square weighted deviation (MSWD)
    Pearson product-moment correlation coefficient
    Regression analysis
    Spearman's rank correlation coefficient
    Student's t-test
    Time series analysis
    Conjoint Analysis

Misuse of statistics
Main article: Misuse of statistics

Misuse of statistics can produce subtle, but serious errors in description and interpretation—subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.

Even when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data—which measures the extent to which a trend could be caused by random variation in the sample—may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.

There is a perception that statistical knowledge is all-too-frequently intentionally misused by finding ways to interpret only the data that are favorable to the presenter.[24] A mistrust and misunderstanding of statistics is associated with the quotation, "There are three kinds of lies: lies, damned lies, and statistics". Misuse of statistics can be both inadvertent and intentional, and the book How to Lie with Statistics[24] outlines a range of considerations. In an attempt to shed light on the use and misuse of statistics, reviews of statistical techniques used in particular fields are conducted (e.g. Warne, Lazo, Ramos, and Ritter (2012)).[25]

Ways to avoid misuse of statistics  using proper diagrams and avoiding bias.[26] Misuse can occur when conclusions are overgeneralized and claimed to be representative of more than they really are, often by either deliberately or unconsciously overlooking sampling bias.[27] Bar graphs are arguably the easiest diagrams to use and understand, and they can be made either by hand or with simple computer programs.[26] Unfortunately, most people do not look for bias or errors, so they are not noticed. Thus, people may often believe that something is true even if it is not well represented.[27] To make data gathered from statistics believable and accurate, the sample taken must be representative of the whole.[28] According to Huff, "The dependability of a sample can be destroyed by [bias]... allow yourself some degree of skepticism."[29]

To assist in the understanding of statistics Huff proposed a series of questions to be asked in each case:[30]

    Who says so? (Does he/she have an axe to grind?)
    How does he/she know? (Does he/she have the resources to know the facts?)
    What’s missing? (Does he/she give us a complete picture?)
    Did someone change the subject? (Does he/she offer us the right answer to the wrong problem?)
    Does it make sense? (Is his/her conclusion logical and consistent with what we already know?)

The confounding variable problem: X and Y may be correlated, not because there is causal relationship between them, but because both depend on a third variable Z. Z is called a confounding factor.
Misinterpretation: correlation

The concept of correlation is particularly noteworthy for the potential confusion it can cause. Statistical analysis of a data set often reveals that two variables (properties) of the population under consideration tend to vary together, as if they were connected. For example, a study of annual income that also looks at age of death might find that poor people tend to have shorter lives than affluent people. The two variables are said to be correlated; however, they may or may not be the cause of one another. The correlation phenomena could be caused by a third, previously unconsidered phenomenon, called a lurking variable or confounding variable. For this reason, there is no way to immediately infer the existence of a causal relationship between the two variables. (See Correlation does not imply causation.)
History of statistical science
Gerolamo Cardano, the earliest pioneer on the mathematics of probability.
Main articles: History of statistics and Founders of statistics

Statistical methods date back at least to the 5th century BC.

Some scholars pinpoint the origin of statistics to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt.[31] Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to  the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.

Its mathematical foundations were laid in the 17th century with the development of the probability theory by Gerolamo Cardano, Blaise Pascal and Pierre de Fermat. Mathematical probability theory arose from the study of games of chance, although the concept of probability was already examined in medieval law and by philosophers such as Juan Caramuel.[32] The method of least squares was first described by Adrien-Marie Legendre in 1805.
Karl Pearson, a founder of mathematical statistics.

The modern field of statistics emerged in the late 19th and early 20th century in three stages.[33] The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions d introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics – height, weight, eyelash length among others.[34] Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment,[35] the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things.[36] Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.[37]

Ronald Fisher coined the term null hypothesis during the Lady tasting tea experiment, which "is never proved or established, but is possibly disproved, in the course of experimentation".[38][39]

The second wave of the 1910s and 20s was initiated by William Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance, which was the first to use the statistical term, variance, his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments,[40][41][42][43] where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information.[44] In his 1930 book The Genetical Theory of Natural Selection he applied statistics to various biological concepts such as Fisher's principle[45]). Nevertheless, A. W. F. Edwards has remarked that it is "probably the most celebrated argument in evolutionary biology".[45] (about the sex ratio), the Fisherian runaway,[46][47][48][49][50][51] a concept in sexual selection about a positive feedback runaway affect found in evolution.

The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of "Type II" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in a better method of estimation than purposive (quota) sampling.[52]

Today, statistical methods are applied in all fields that decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations, and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze Big data.[53]
Applications
Applied statistics, theoretical statistics and mathematical statistics

"Applied statistics" comprises descriptive statistics and the application of inferential statistics.[54][55] Theoretical statistics concerns both the logical arguments underlying justification of approaches to statistical inference, as well encompassing mathematical statistics. Mathematical statistics s not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.
Machine learning and data mining

There are two applications for machine learning and data mining: data management and data analysis. Statistics tools are necessary for the data analysis.
Statistics in society

Statistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Statistical consultants can help organizations and companies that don't have in-house expertise relevant to their particular questions.
Statistical computing
gretl, an example of an open source statistical package
Main article: Computational statistics

The rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.

Increased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with new emphasis on "experimental" and "empirical" statistics. A large number of both and special purpose statistical software are now available.
Statistics applied to mathematics or the arts

Traditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was "required learning" in most sciences. This has changed with use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically.[according to whom?] Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.

    In number theory, scatter plots of data generated by a distribution function may be transformed with familiar tools used in statistics to reveal underlying patterns, which may then lead to hypotheses.
    Methods of statistics including predictive methods in forecasting are combined with chaos theory and fractal geometry to create video works that are considered to have great beauty.[citation needed]
    The process art of Jackson Pollock relied on artistic experiments whereby underlying distributions in nature were artistically revealed.[citation needed] With the advent of computers, statistical methods were applied to formalize such distribution-driven natural processes to make and analyze moving video art.[citation needed]
    Methods of statistics may be used predicatively in performance art, as in a card trick based on a Markov process that only works some of the time, the occasion of which can be predicted using statistical methodology.
    Statistics can be used to predicatively create art, as in the statistical or stochastic music invented by Iannis Xenakis, where the music is performance-specific. Though this type of artistry does not always come out as expected, it does behave in ways that are predictable and tunable using statistics.

Specialized disciplines
Main article: List of fields of application of statistics

Statistical techniques are used in a wide range of types of scientific and social research, including: biostatistics, computational biology, computational sociology, network biology, social science, sociology and social research. Some fields of inquiry use applied statistics so extensively that they have specialized terminology. These disciplines :

    Actuarial science (assesses risk in the insurance and finance industries)
    Applied information economics
    Astrostatistics (statistical evaluation of astronomical data)
    Biostatistics
    Business statistics
    Chemometrics (for analysis of data from chemistry)
    Data mining (ing statistics and pattern recognition to discover knowledge from data)
    Data science
    Demography
    Econometrics (statistical analysis of economic data)
    Energy statistics
    Engineering statistics
    Epidemiology (statistical analysis of disease)
    Geography and Geographic Information Systems, specifically in Spatial analysis
    Image processing
    Medical Statistics
    Psychological statistics
    Reliability engineering
    Social statistics
    Statistical Mechanics

In addition, there are particular types of statistical analysis that have also developed their own specialised terminology and methodology:

    Bootstrap / Jackknife resampling
    Multivariate statistics
    Statistical classification
    Structured data analysis (statistics)
    Structural equation modelling
    Survey methodology
    Survival analysis
    Statistics in various sports, particularly baseball - known as Sabermetrics - and cricket

Statistics form a key basis tool in business and manufacturing as well. It is used to understand measurement systems variability, control processes (as in statistical process control or SPC), for summarizing data, and to make data-driven decisions. In these roles, it is a key tool, and perhaps the only reliable tool.
See also
Library resources about
Statistics

    Resources in your library 

Main article: Outline of statistics

    Abundance estimation
    Data science
    Glossary of probability and statistics
    List of academic statistical associations
    List of important publications in statistics
    List of national and international statistical services
    List of statistical packages (software)
    List of statistics articles
    List of university statistical consulting centers
    Notation in probability and statistics

Foundations and major areas of statistics

    Foundations of statistics
    List of statisticians
    Official statistics
    Multivariate analysis of variance

Abundance estimation comprises all statistical methods for estimating the number of individuals in a population. In ecology, this may be anything from estimating the number of daisies in a field to estimating the number of blue whales in the ocean.[1]

Contents

    1 Plot sampling
    2 Removal, catch-effort and change-in-ratio
    3 Mark-recapture
    4 Distance sampling
    5 References

Plot sampling
Main article: Plot sampling
Removal, catch-effort and change-in-ratio
[icon] 	This section is empty. You can help by adding to it. (March 2014)
Mark-recapture
Main article: Mark-recapture
Distance sampling
Main article: Distance sampling


alternative hypothesis

atomic event
    Another name for elementary event

B

bar chart

bias
    1.  A sample that is not representative of the population
    2.  The difference between the expected value of an estimator and the true value

binary data
    Data that can take only two values, usually represented by 0 and 1

binomial distribution

bivariate analysis

box plot

C

causal study
    A statistical study in which the objective is to measure the effect of some variable on an outcome relative to a different variable. For example, how will my headache feel if I take aspirin, versus if I do not take aspirin? Causal studies may be either experimental or observational.[1]

central limit theorem

chi-squared distribution

chi-squared test

concomitants
    In a statistical study, concomitants are any variables whose values are unaffected by treatments, such as a unit’s age, gender, and cholesterol level before starting a diet (treatment).[1]

conditional distribution
    Given two jointly distributed random variables X and Y, the conditional probability distribution of Y given X (written "Y | X") is the probability distribution of Y when X is known to be a particular value

conditional probability
    The probability of some event A, assuming event B. Conditional probability is written P(A|B), and is read "the probability of A, given B"

confidence interval
    In inferential statistics, a CI is a range of plausible values for the population mean.[2] For example, based on a study of sleep habits among 100 people, a researcher may estimate that the overall population sleeps somewhere between 5 and 9 hours per night. This is different from the sample mean, which can be measured directly.

confidence level
    Also known as a confidence coefficient, the confidence level indicates the probability that the confidence interval (range) captures the true population mean. For example, a confidence interval with a 95 percent confidence level has a 95 percent chance of capturing the population mean. Technically, this means that, if the experiment were repeated many times, 95 percent of the CIs would contain the true population mean.[2]

continuous variable

correlation
    Also called correlation coefficient, a numeric measure of the strength of linear relationship between two random variables (one can use it to quantify, for example, how shoe size and height are correlated in the population). An example is the Pearson product-moment correlation coefficient, which is found by dividing the covariance of the two variables by the product of their standard deviations. Independent variables have a correlation of 0

count data
    Data arising from counting that can take only non-negative integer values

covariance
    Given two random variables X and Y, with expected values E ( X ) = μ {\displaystyle E(X)=\mu } E(X)=\mu and E ( Y ) = ν {\displaystyle E(Y)=\nu } E(Y)=\nu, covariance is defined as the expected value of random variable ( X − μ ) ( Y − ν ) {\displaystyle (X-\mu )(Y-\nu )} (X - \mu) (Y - \nu), and is written cov ⁡ ( X , Y ) {\displaystyle \operatorname {cov} (X,Y)} \operatorname{cov}(X, Y). It is used for measuring correlation

D

data

data analysis

data set
    A sample and the associated data points

data point
    A typed measurement — it can be a Boolean value, a real number, a vector (in which case it's also called a data vector), etc

degrees of freedom

dependent variable

descriptive statistics

deviation

discrete variable

dot plot

double counting

E

elementary event
    An event with only one element. For example, when pulling a card out of a deck, "getting the jack of spades" is an elementary event, while "getting a king or an ace" is not

estimator
    A function of the known data that is used to estimate an unknown parameter; an estimate is the result from the actual application of the function to a particular set of data. The mean can be used as an estimator

expected value
    The sum of the probability of each possible outcome of the experiment multiplied by its payoff ("value"). Thus, it represents the average amount one "expects" to win per bet if bets with identical odds are repeated many times. For example, the expected value of a six-sided die roll is 3.5. The concept is similar to the mean. The expected value of random variable X is typically written E(X) for the operator and μ {\displaystyle \mu } \mu (mu) for the parameter

experiment
    Any procedure that can be infinitely repeated and has a well-defined set of outcomes

event
    A subset of the sample space (a possible experiment's outcome), to which a probability can be assigned. For example, on rolling a die, "getting a five or a six" is an event (with a probability of one third if the die is fair)

F

frequency distribution

G

grouped data

H

histogram

I

independent variable

J

joint distribution
    Given two random variables X and Y, the joint distribution of X and Y is the probability distribution of X and Y together

joint probability
    The probability of two events occurring together. The joint probability of A and B is written P ( A ∩ B ) {\displaystyle P(A\cap B)} P(A\cap B) or P ( A ,   B ) . {\displaystyle P(A,\ B).} P(A, \ B).


K

kurtosis
    A measure of the "peakedness" of the probability distribution of a real-valued random variable. Higher kurtosis means more of the variance is due to infrequent extreme deviations, as opposed to frequent modestly sized deviations

L

likelihood function
    A conditional probability function considered a function of its second argument with its first argument held fixed. For example, imagine pulling a numbered ball with the number k from a bag of n balls, numbered 1 to n. Then you could describe a likelihood function for the random variable N as the probability of getting k given that there are n balls : the likelihood will be 1/n for n greater or equal to k, and 0 for n smaller than k. Unlike a probability distribution function, this likelihood function will not sum up to 1 on the sample space


M

marginal distribution
    Given two jointly distributed random variables X and Y, the marginal distribution of X is simply the probability distribution of X ignoring information about Y

marginal probability
    The probability of an event, ignoring any information about other events. The marginal probability of A is written P(A). Contrast with conditional probability

mean
    1.  The expected value of a random variable
    2.  The arithmetic mean is the average of a set of numbers, or the sum of the values divided by the number of values

mode

multimodal distribution

multivariate random variable
    A vector whose components are random variables on the same probability space

mutual exclusivity

mutual independence
    A collection of events is mutually independent if for any subset of the collection, the joint probability of all events occurring is equal to the product of the joint probabilities of the individual events. Think of the result of a series of coin-flips. This is a stronger condition than pairwise independence

N

non-sampling error

normal distribution

null hypothesis
    The statement being tested in a test of statistical significance Usually the null hypothesis is a statement of 'no effect' or 'no difference'."[3] For example, if one wanted to test whether light has an effect on sleep, the null hypothesis would be that there is no effect. It is often symbolized as H0.

O

outlier

P

pairwise independence
    A pairwise independent collection of random variables is a set of random variables any two of which are independent

parameter
    Can be a population parameter, a distribution parameter, an unobserved parameter (with different shades of meaning). In statistics, this is often a quantity to be estimated

percentile

pie chart

point estimation

prior probability
    In Bayesian inference, this represents prior beliefs or other information that is available before new data or observations are taken into account

population parameter
    See parameter

posterior probability
    The result of a Bayesian analysis that encapsulates the combination of prior beliefs or information with observed data

probability

probability density
    Describes the probability in a continuous probability distribution. For example, you can't say that the probability of a man being six feet tall is 20%, but you can say he has 20% of chances of being between five and six feet tall. Probability density is given by a probability density function. Contrast with probability mass

probability density function
    Gives the probability distribution for a continuous random variable

probability distribution
    A function that gives the probability of all elements in a given space: see List of probability distributions

probability measure
    The probability of events in a probability space

probability plot

probability space
    A sample space over which a probability measure has been defined

Q

quantile

quartile

R

random variable
    A measurable function on a probability space, often real-valued. The distribution function of a random variable gives the probability of different results. We can also derive the mean and variance of a random variable

See also: Discrete random variable and Continuous random variable


range
    The length of the smallest interval which contains all the data

responses
    In a statistical study, any variables whose values may have been affected by the treatments, such as cholesterol levels after following a particular diet for six months.[1]

S

sample
    That part of a population which is actually observed

sample mean
    The arithmetic mean of a sample of values drawn from the population. It is denoted by x ¯ {\displaystyle {\overline {x}}} {\overline {x}}. An example is the average test score of a subset of 10 students from a class. Sample mean is used as an estimator of the population mean, which in this example would be the average test score of all of the students in the class.

sample space
    The set of possible outcomes of an experiment. For example, the sample space for rolling a six-sided die will be {1, 2, 3, 4, 5, 6}

sampling
    A process of selecting observations to obtain knowledge about a population. There are many methods to choose on which sample to do the observations

sampling distribution
    The probability distribution, under repeated sampling of the population, of a given statistic

sampling error

scatter plot

simple random sample

skewness
    A measure of the asymmetry of the probability distribution of a real-valued random variable. Roughly speaking, a distribution has positive skew (right-skewed) if the higher tail is longer and negative skew (left-skewed) if the lower tail is longer (confusing the two is a common error)

spaghetti plot

standard deviation
    The most commonly used measure of statistical dispersion. It is the Square root of the variance, and is generally written σ {\displaystyle \sigma } \sigma (Sigma)

standard error

standard score

statistic
    The result of a statistical algorithm to a data set. It can also be described as an observable random variable

statistical graphics

statistical hypothesis testing

statistical independence
    Two events are independent if the outcome of one does not affect that of the other (for example, getting a 1 on one die roll does not affect the probability of getting a 1 on a second roll). Similarly, when we assert that two random variables are independent, we intuitively mean that knowing something about the value of one of them does not yield any information about the value of the other

statistical inference
    Inference about a population from a random sample drawn from it or, more generally, about a random process from its observed behavior during a finite period of time

statistical model

statistical population
    A set of entities about which statistical inferences are to be drawn, often based on random sampling. One can also talk about a population of measurements or values

statistical dispersion
    Statistical variability is a measure of how diverse some data is. It can be expressed by the variance or the standard deviation

statistical parameter
    A parameter that indexes a family of probability distributions

statistical significance

statistics

stem-and-leaf display

symmetric probability distribution

systematic sampling

T

treatments
    Variables in a statistical study that are conceptually manipulable. For example, in a health study, following a certain diet is a treatment whereas age is not.[1]

trial
    Can refer to each individual repetition when talking about an experiment composed of any fixed number of them. As an example, one can think of an experiment being any number from one to n coin tosses, say 17. In this case, one toss can be called a trial to avoid confusion, since the whole experiment is composed of 17 ones.

U

units
    In a statistical study, the objects to which treatments are assigned. For example, in a study examining the effects of smoking cigarettes, the units would be people.[1]

V

variance
    A measure of its statistical dispersion of a random variable, indicating how far from the expected value its values typically are. The variance of random variable X is typically designated as var ⁡ ( X ) {\displaystyle \operatorname {var} (X)} \operatorname{var}(X), σ X 2 {\displaystyle \sigma _{X}^{2}} \sigma_X^2, or simply σ 2 {\displaystyle \sigma ^{2}} \sigma ^{2}

W
X
Y
Z
See also

    Notation in probability and statistics
    Probability axioms
    Glossary of experimental design
    List of statistical topics
    List of probability topics
    Glossary of areas of mathematics

    Statistical software are specialized computer programs for analysis in statistics and econometrics.

Contents

    1 Open-source
    2 Public domain
    3 Freeware
    4 Proprietary
        4.1 Add-ons
    5 See also
    6 References
    7 External links

Open-source
gretl is an example of an open-source statistical package

    ADaMSoft – a generalized statistical software with data mining algorithms and methods for data management
    ADMB – a software suite for non-linear statistical modeling based on C++ which uses automatic differentiation
    AMoreAccurateFourierTransform - software for computing Fourier transforms - sourceforge.net/projects/amoreaccuratefouriertransform/ 
    [1]
    Bayesian Filtering Library
    Chronux – for neurobiological time series data
    CBEcon – web-based econometrics and statistical software
    DataMelt (DMelt) – Java-based statistical analysis framework for scientists and engineers. It s an IDE
    DAP – free replacement for SAS
    Environment for DeveLoping KDD-Applications Supported by Index-Structures (ELKI) a software framework for developing data mining algorithms in Java
    Fityk – nonlinear regression software (GUI and command line)
    GNU Octave – programming language very similar to MATLAB with statistical features
    gretl – gnu regression, econometrics and time-series library
    intrinsic Noise Analyzer (iNA) – For analyzing intrinsic fluctuations in biochemical systems
    JASP – A free software alternative to IBM SPSS Statistics with additional option for Bayesian methods
    Just another Gibbs sampler (JAGS) – a program for analyzing Bayesian hierarchical models using Markov chain Monte Carlo developed by Martyn Plummer. It is similar to WinBUGS
    JMulTi
    LIBSVM – C++ support vector machine libraries
    MLPACK (C++ library) – open-source library for machine learning, exploits C++ language features to provide maximum performance and flexibility while providing a simple and consistent application programming interface (API)
    Mondrian – data analysis tool using interactive statistical graphics with a link to R
    Neurophysiological Biomarker Toolbox - Matlab toolbox for data-mining of neurophysiological biomarkers
    OpenBUGS
    OpenEpi – A web-based, open-source, operating-independent series of programs for use in epidemiology and statistics based on JavaScript and HTML
    OpenNN – A software library written in the programming language C++ which implements neural networks, a main area of deep learning research
    OpenMx – A package for structural equation modeling running in R (programming language)
    Orange, a data mining, machine learning, and bioinformatics software
    Pandas – High-performance computing (HPC) data structures and data analysis tools for Python in Python and Cython (statsmodels, scikit-learn)
    Perl Data Language – Scientific computing with Perl
    Ploticus – software for generating a variety of graphs from raw data
    PSPP – A free software alternative to IBM SPSS Statistics
    R – free implementation of the S (programming language)
        Programming with Big Data in R (pbdR) – a series of R packages enhanced by SPMD parallelism for big data analysis
        R Commander – GUI interface for R
        Rattle GUI – GUI interface for R
        Revolution Analytics – production-grade software for the enterprise big data analytics
        RStudio – GUI interface and development environment for R
    ROOT – an open-source C++ system for data storage, processing and analysis, developed by CERN and used to find the Higgs boson
    Salstat - menu-driven statistics software
    Scilab – uses GPL-compatible CeCILL license
    SciPy – Python library for scientific computing that contains the stats sub-package which is partly based on the venerable |STAT (a.k.a. PipeStat, formerly UNIX|STAT) software
        scikit-learn - extends SciPy with a host of machine learning models (classification, clustering, regression, etc.)
        statsmodels - extends SciPy with statistical models and tests (regression, plotting, example datasets, generalized linear model (GLM), time series analysis, autoregressive–moving-average model (ARMA), vector autoregression (VAR), non-parametric statistics, ANOVA, empirical likelihood)
    Shogun (toolbox) – open-source, large-scale machine learning toolbox that provides several SVM (Support Vector Machine) implementations (like libSVM, SVMlight) under a common framework and interfaces to Octave, MATLAB, Python, R
    Simfit – simulation, curve fitting, statistics, and plotting
    SOCR
    SOFA Statistics – desktop GUI program focused on ease of use, learn as you go, and beautiful output
    Stan (software) – open-source package for obtaining Bayesian inference using the No-U-Turn sampler, a variant of Hamiltonian Monte Carlo. It’s somewhat like BUGS, but with a different language for expressing models and a different sampler for sampling from their posteriors
    Statistical Lab – R-based and focusing on educational purposes
    Torch (machine learning) – a deep learning software library written in Lua (programming language)
    Weka (machine learning) – a suite of machine learning software written at the University of Waikato
    Xlisp-stat

Public domain

    CSPro
    Epi Info
    X-12-ARIMA

Freeware

    BV4.1
    GeoDA
    MaxStat Lite – statistical software
    MINUIT
    WinBUGS – Bayesian analysis using Markov chain Monte Carlo methods
    Winpepi – package of statistical programs for epidemiologists

Proprietary

    Analytica - visual analytics and statistics package
    Angoss - products KnowledgeSEEKER and KnowledgeSTUDIO incorporate several data mining algorithms
    ASReml – for restricted maximum likelihood analyses
    BMDP – statistics package
    Data Applied – for building statistical models
    DB Lytix - 800+ in-database models
    EViews – for econometric analysis
    FAME (database) – a system for managing time-series databases
    GAUSS – programming language for statistics
    Genedata – software solution for integration and interpretation of experimental data in the life science R&D
    GenStat – statistics package
    GLIM – early package for fitting generalized linear models
    GraphPad InStat – very simple with lots of guidance and explanations
    GraphPad Prism – biostatistics and nonlinear regression with clear explanations
    Intellectus Statistics 
    -Easy to use with an intuitive user interface. Output formatted as a narrative interpretation of the analyses complete with APA tables and figures.
    IMSL Numerical Libraries – software library with statistical algorithms
    JMP – visual analysis and statistics package
    LIMDEP – comprehensive statistics and econometrics package
    LISREL – statistics package used in structural equation modeling
    Maple – programming language with statistical features
    Mathematica – a software package with statistical particularlyŋ features
    MATLAB – programming language with statistical features
    MaxStat Pro – statistical software
    MedCalc – for biomedical sciences
    Microfit – econometrics package, time series
    Minitab – statistics package
    MLwiN – multilevel models (free to UK academics)
    NAG Numerical Library – comprehensive math and statistics library
    Neural Designer – commercial deep learning package
    NCSS – statistics package
    NLOGIT – comprehensive statistics and econometrics package
    NMath Stats – statistical package for .NET Framework
    O-Matrix – programming language
    OriginPro – statistics and graphing, programming access to NAG library
    PASS Sample Size Software (PASS) – power and sample size software from NCSS
    Partek – statistics package with specific applications for genomic, HTS, and QSAR data
    Plotly – plotting library and styling interface for analyzing data and creating browser-based graphs. Available for R, Python, MATLAB, Julia, and Perl
    Primer-E Primer – environmental and ecological specific
    PV-WAVE – programming language comprehensive data analysis and visualization with IMSL statistical package
    Qlucore Omics Explorer - interactive and visual data analysis software
    Quantum Programming Language – part of the SPSS MR product line, mostly for data validation and tabulation in Marketing and Opinion Research
    RapidMiner – machine learning toolbox
    Regression Analysis of Time Series (RATS) – comprehensive econometric analysis package
    SAS (software) – comprehensive statistical package
    SHAZAM (Econometrics and Statistics Software) – comprehensive econometrics and statistics package
    Simul - econometric tool for multidimensional (multi-sectoral, multi-regional) modeling
    SigmaStat – package for group analysis
    SmartPLS - statistics package used in partial least squares path modeling (PLS) and PLS-based structural equation modeling
    SOCR – online tools for teaching statistics and probability theory
    Speakeasy (computational environment) – numerical computational environment and programming language with many statistical and econometric analysis features
    SPSS Modeler – comprehensive data mining and text analytics workbench
    SPSS Statistics – comprehensive statistics package that stands for "Statistical Package for the Social Sciences"
    Stata – comprehensive statistics package
    Statgraphics – statistics package to  cloud computing and Six Sigma for use in business development, process improvement, data visualization and statistical analysis, design of experiment, point processes, geospatial analysis, regression, and time series analysis are all d within this complete statistical package.
    Statistica – comprehensive statistics package
    StatsDirect – statistics package designed for biomedical, public health and health science uses
    StatXact – package for exact nonparametric and parametric statistics
    Systat – statistics package
    SuperCROSS - comprehensive statistics package with ad-hoc, cross tabulation analysis
    S-PLUS – statistics package
    Unistat – statistics package that can also work as Excel add-in
    The Unscrambler - free-to-try commercial multivariate analysis software for Windows
    Wolfram Language[2] - the computer language that evolved from the program Mathematica. It has similar statistical capabilities as Mathematica.
    World Programming System (WPS) – statistical package that supports the SAS language
    XploRe

Add-ons

    Analyse-it – add-on to Microsoft Excel for statistical analysis
    NumXL – add-on to Microsoft Excel for statistical and time series analysis
    QI Macros - SPC and Lean Six Sigma add-on to Microsoft Excel, for both PC and Mac
    SigmaXL – add-on to Microsoft Excel for statistical and graphical analysis
    SPC XL – add-on to Microsoft Excel for statistics
    Statgraphics Sigma Express - add-on to Microsoft Excel for Six Sigma statistical analysis
    SUDAAN – add-on to SAS and SPSS for statistical surveys
    XLfit add-on to Microsoft Excel for curve fitting and statistical analysis
    XLStat - add-on to Microsoft Excel for statistical analyses.

See also

    Comparison of statistical packages
    Econometric software
    Free statistical software
    List of computer algebra systems
    List of graphing software
    List of numerical analysis software
    List of numerical libraries
    Mathematical software
    Psychometric software
Correlation

Correlation is a statistical technique that can show whether and how strongly pairs of variables are related. For example, height and weight are related; taller people tend to be heavier than shorter people. The relationship isn't perfect. People of the same height vary in weight, and you can easily think of two people you know where the shorter one is heavier than the taller one. Nonetheless, the average weight of people 5'5'' is less than the average weight of people 5'6'', and their average weight is less than that of people 5'7'', etc. Correlation can tell you just how much of the variation in peoples' weights is related to their heights.

Although this correlation is fairly obvious your data may contain unsuspected correlations. You may also suspect there are correlations, but don't know which are the strongest. An intelligent correlation analysis can lead to a greater understanding of your data.
Techniques in Determining Correlation

There are several different correlation techniques. The Survey System's optional Statistics Module s the most common type, called the Pearson or product-moment correlation. The module also s a variation on this type called partial correlation. The latter is useful when you want to look at the relationship between two variables while removing the effect of one or two other variables.

Like all statistical techniques, correlation is only appropriate for certain kinds of data. Correlation works for quantifiable data in which numbers are meaningful, usually quantities of some sort. It cannot be used for purely categorical data, such as gender, brands purchased, or favorite color.
Rating Scales

Rating scales are a controversial middle case. The numbers in rating scales have meaning, but that meaning isn't very precise. They are not like quantities. With a quantity (such as dollars), the difference between 1 and 2 is exactly the same as between 2 and 3. With a rating scale, that isn't really the case. You can be sure that your respondents think a rating of 2 is between a rating of 1 and a rating of 3, but you cannot be sure they think it is exactly halfway between. This is especially true if you labeled the mid-points of your scale (you cannot assume "good" is exactly half way between "excellent" and "fair").

Most statisticians say you cannot use correlations with rating scales, because the mathematics of the technique assume the differences between numbers are exactly equal. Nevertheless, many survey researchers do use correlations with rating scales, because the results usually reflect the real world. Our own position is that you can use correlations with rating scales, but you should do so with care. When working with quantities, correlations provide precise measurements. When working with rating scales, correlations provide indications.
Correlation Coefficient

The main result of a correlation is called the correlation coefficient (or "r"). It ranges from -1.0 to +1.0. The closer r is to +1 or -1, the more closely the two variables are related.

If r is close to 0, it means there is no relationship between the variables. If r is positive, it means that as one variable gets larger the other gets larger. If r is negative it means that as one gets larger, the other gets smaller (often called an "inverse" correlation).

While correlation coefficients are normally reported as r = (a value between -1 and +1), squaring them makes then easier to understand. The square of the coefficient (or r square) is equal to the percent of the variation in one variable that is related to the variation in the other. After squaring r, ignore the decimal point. An r of .5 means 25% of the variation is related (.5 squared =.25). An r value of .7 means 49% of the variance is related (.7 squared = .49).

A correlation can also show a second result of each test - statistical significance. In this case, the significance level will tell you how likely it is that the correlations reported may be due to chance in the form of random sampling error. If you are working with small sample sizes, choose a format that s the significance level. This format also reports the sample size.

A key thing to remember when working with correlations is never to assume a correlation means that a change in one variable causes a change in another. Sales of personal computers and athletic shoes have both risen strongly in the last several years and there is a high correlation between them, but you cannot assume that buying computers causes people to buy athletic shoes (or vice versa).

The second caveat is that the Pearson correlation technique works best with linear relationships: as one variable gets larger, the other gets larger (or smaller) in direct proportion. It does not work well with curvilinear relationships (in which the relationship does not follow a straight line). An example of a curvilinear relationship is age and health care. They are related, but the relationship doesn't follow a straight line. Young children and older people both tend to use much more health care than teenagers or young adults. Multiple regression (also d in the Statistics Module) can be used to examine curvilinear relationships, but it is beyond the scope of this article.