Big Data Analytics platforms
Big Data tools Hadoop, Spark
Distributed computing tools a plus Spark, MapReduce, Hadoop, Hive
Real time and streaming analytics systems Flume, Kafka, Storm
Hadoop Ecosystem/platform
Spotfire
Azure Data Analytics platforms HDInsight, APS and PDW
Amazon Data Analytics platform Kinesis, EMR
Other cloud based Data Analytics platforms HortonWorks, Vertica LexisNexis HPCC System
hadoop
spark
YARN
HDFS
MapReduce
Apache Flink
Apache Storm
Apache Samza
Apache Kafka
Cloudera
software framework
big data framework

Bigdata Platforms and Bigdata Analytics Software focuses on providing efficient analytics for extremely large datasets. These analytics helps the organisations to gain insight, by turning data into high quality information, providing deeper insights about the business situation. This enables the business to take advantage of the digital universe. IBM Bigdata Analytics, HP Bigdata , SAP Bigdata Analytics, Microsoft Bigdata, Oracle Bigdata Analytics, Talend Open Studio, Teradata Bigdata Analytics, SAS Big data, Dell Bigdata Analytics, HPCC System Big data, Palantir Bigdata, Pivotal Bigdata, Google BigQuery, Pentaho Big Data Analytics, Amazon Web Service, Cloudera Enterprise Bigdata, Hortonworks Data Platform, FICO Bigdata Analytics, Cisco Bigdata, Splunk Bigdata Analytics, Fusion-io Bigdata, Intel Bigdata, Mu Sigma Bigdata, MicroStrategy Bigdata , Opera Solutions Bigdata, Redhat Bigdata, Informatica Bigdata, MarkLogic Bigdata, Vmware Bigdata, Syncsort Bigdata, SGI Bigdata, MongoDB , Guavus Bigdata, Alteryx Bigdata, 1010data Advanced Analytics, Actian Analytics Platform, MapR, Tableau Software bigdata, QlikView Bigdata, Attivio’s Bigdata, DataStax Bigdata, Gooddata, Google Bigdata, Datameer, CSC Big Data Platform, Flytxt, Amdocs, Cisco Bigdata, Platfora and GE Bigdata are some of the Big data Analytics Platforms and Software in no particular order.
1. IBM Bigdata Analytics

IBM Bigdata Analytics solution portfolio  InfoSphere Streams , InfoSphere BigInsights , IBM Watson Explorer , IBM PureData powered by Netezza technology , DB2 with BLU Acceleration , IBM Smart Analytics System , InfoSphere Information Server and InfoSphere Master Data Management.2. HP Bigdata

HP’s Bigdata Analytics solution  HP HAVEn and HP Vertica. HP HAVEn is a platform comprised of software, services, and hardware. Big Data of any type either structured and unstructured can be analyzed to lead to powerful strategic insights. HP Vertica Dragline let organizations store their data in a cost effective manner, and provide capabilities to explore it quickly using SQL based tools

3. SAP Bigdata Analytics

SAP Bigdata Analytics platform  In Memory Platform called, SAP HANA, and SAP IQ, which is a column oriented, grid based, massively parallel processing database. There is also SAP HANA platform and Apache Hadoop solution available together. Bigdata Analytics solutions include the Predictive Analytics and Text Analytics solutions.




4. Microsoft Bigdata

Microsoft Azure is an open and flexible cloud platform which enables to quickly build, deploy and manage applications across a global network of Microsoft-managed datacenters. The applications can be  using any language, tool or framework and can integrated with other public cloud applications in the IT environment.



5. Oracle Bigdata Analytics

Oracle Bigdata Analytics solutions include Oracle Big Data Appliance, Oracle Exadata Database Machine and Oracle Exalytics In-Memory Machine. These are engineered Systems which are pre-integrated to reduce the cost and complexity of IT infrastructures. The database include Oracle Database, Oracle NoSQL Database, MySQL and MySQL Cluster, Oracle Event Processing, Oracle NoSQL Database and Oracle Coherence, Oracle Endeca Information  and in database analytics.




6. Talend Open Studio

Talend Open Studio is a versatile set of open source products for developing, testing, deploying and administrating data management and application integration projects. Talend delivers the only unified platform that makes data management and application integration easier by providing a unified environment for managing the entire lifecycle across enterprise boundaries.
Talend’s products dramatically lower the adoption barrier for businesses wanting powerful packaged solutions to operational challenges like data cleansing, master data management, and enterprise service bus deployment. Leveraging and extending leading Apache technologies, Talend’s open source ESB and open source SOA solutions help organizations to build flexible, high-performance enterprise architectures that integrate and service-enable distributed applications.


7. Teradata Bigdata Analytics

Teradata has  a simple architecture called, the Unified Data Architecture in Bigdata Analytics. The Teradata Aster  Platform ease the  of crucial business insights from all data types. With its powerful analytic applications coupled with minimal time and effort requirements, it provides the  insights needed for sophisticated companies today.

8. SAS Bigdata Analytics

SAS Bigdata Analytics solution portfolio  Credit Scoring for SAS Enterprise Miner, SAS High-Performance Data Mining, SAS Model Manager, SAS Scoring Accelerator, SAS Text Miner and SAS Visual Statistics.

9. Dell Bigdata Analytics

Dell Bigdata Analytics  Kitenga Analytics Suite, Boomi AtomSphere and SharePlex Connector for Hadoop. Kitenga Analytics Suite provides you with integrated information modeling and visualization capabilities in a big data search and business analytics platform.

10. HPCC Systems Big data

HPCC Systems is an Open-source platform for Big Data analysis. The Data Refinery engine called Thor, clean, link, transform and analyze Big Data. Thor supports ETL (Extraction, Transformation and Loading) functions like ingesting unstructured/structured data out, data profiling, data hygiene, and data linking out of the box. The Data Delivery engine (Roxie) provides highly concurrent and low latency real time query capability. The Thor processed data can be accessed by a large number of users concurrently in real time fashion using the Roxie. The programming language, Enterprise Control Language (ECL), is used to program both the data processing jobs on Thor and the queries on Roxie.

HPCC Systems is an Open-source platform for Big Data analysis.

HPCC Systems is an Open-source platform for Big Data analysis.
11. Palantir Bigdata

Palantir Bigdata solution  Palantir Gotham to integrate, manage, secure, and analyze all of the enterprise data and Palantir Metropolis to ntegrate, enrich, model, and analyze any kind of quantitative data.

12. Pivotal Bigdata

Pivotal Big Data solutions help to discover insight from all data to build applications that serve customers in the context to store, manage, and deliver value from , massive data sets using the most disruptive set of enterprise data products such as MPP and column store databases, in-memory data processing, and Hadoop.

13. Google BigQuery

Google BigQuery is a web service that enables companies to analyze massive datasets using Google’s infrastructure . This can analyze up to billions of rows in seconds. It is scalable and easy to use with the the familiar SQL query language. BigQuery lets developers and businesses tap into powerful data analytics on demand against multi-terabyte datasets in seconds.

14. Pentaho Big Data Analytics

Pentaho Big Data Analytics s a comprehensive and unified solution that supports the entire big data lifecycle. Regardless of the data source, within a single platform the solution provides visual big data analytics tools to extract and prepare the data plus the visualizations and analytics. The Open, standards based architecture, make it easy to integrate with or extend existing infrastructure.

15. Amazon Web Service

Amazon Web Services provides cloud based analytics services to help you process and analyze any volume of data, whether your need is for managed Hadoop clusters, real-time streaming data, petabyte scale data warehousing, or orchestration.

16. Cloudera Enterprise Bigdata

Cloudera Enterprise  CDH, the open source Hadoop-based platform, as well as advanced system management and data management tools plus dedicated support and community advocacy .

17. Hortonworks Data Platform

HDP is a platform for multi-workload data processing across an array of processing methods – from batch through interactive to real-time – all supported with solutions for governance, integration, security and operations.

18. FICO Bigdata Analytics

FICO s comprehensive Big Data Analytics software solutions, Predictive Analytics and Business Intelligence tools  FICO Data Orchestrator, FICO Decision Management Platform, FICO Decision Optimizer, FICO Model Builder, FICO Model Central Solution, FICO Predictive Analytics and FICO Solution Stack.

19. Cisco Bigdata

Cisco UCS Common Platform Architecture (CPA) for big data  computing, storage, connectivity, and unified management capabilities. Unique to this architecture are transparent, simplified data and management integration with an enterprise application ecosystem.

20. Splunk Bigdata Analytics

Splunk s a portfolio of Bigdata Analytics software such as Hunk: Splunk Analytics for Hadoop, NoSQL Data Stores, Splunk Hadoop Connect, Hadoop Management and Splunk DB Connect.

21. Fusion-io Bigdata

Fusion-io solutions eliminate the random workload performance deficiencies common to MongoDB, Cassandra and NoSQL databases, such as HBASE, while reducing the operational overhead of their conventional scale out architectures. Fusion based solutions deliver predictable and consistently high performance across the entire database, resulting in a more efficient overall system that can require fewer nodes, less DRAM, and use less energy for power and cooling.

22. Intel Bigdata

Intel portfolio  technology products such as Intel Xeon processors, 10 Gigabit server adapters, SSDs, and the Intel Distribution improve performance for big data projects.

23. Mu Sigma Bigdata

Mu Sigma’s platforms for Data Sciences include muXo, muHPC and muText. muXo is an advanced decision optimization engine  to solve complex business problems. It provides a suite of constantly evolving, cutting-edge meta-heuristic algorithms. muHPC is a suite of popular statistical algorithms, integrated in the form of R packages, for Big Data analysis. Written in MapReduce, muHPCTM algorithms leverage the power of parallel computation. Mu Sigma’s text mining engine enables knowledge  from unstructured and semi-structured data .

24. MicroStrategy Bigdata

MicroStrategy Bigdata solution called PRIME, which is deployed on the Cloud, provides visualization and dashboarding engine with an innovative massively parallel in-memory data store. This architecture allows companies to rapidly build and deploy powerful information-driven apps that deliver analytics to hundreds of thousands of users in a fraction of the time and cost of other approaches.

25. Opera Solutions Bigdata

Opera Solutions Bigdata solution Vektor Big Data analytics and Signal-processing platform integrates Big Data flows from both inside and outside the enterprise; provides the technology to identify, extract, and store Signals; and supports deployment of all Signal Apps.

26. Redhat Bigdata

Majority of big data implementations run on Linux. Red Hat Enterprise Linux is a leading platform for big data deployments. Red Hat Enterprise Linux excels in distributed architectures and  features that  critical big data needs. Managing tremendous data volumes and intensive analytic processing requires an infrastructure  for high performance, reliability, fine-grained resource management, and scale-out storage.

27. Informatica Bigdata

Informatica PowerCenter Big Data Edition provides a safe, efficient way to integrate all types of data on Hadoop at any scale without having to learn Hadoop.

28. MarkLogic Bigdata

MarkLogic Bigdata solution the Enterprise NoSQL database, brings all the features into one unified system: a document-centric, schema-agnostic, structure-aware, clustered, transactional, secure, database server with -in search and a full suite of application services.

29. Vmware Bigdata

vSphere is a robust, high-performance virtualization layer that abstracts server hardware resources and makes them shareable by multiple virtual machines. Runs Hadoop workloads on vSphere to achieve higher utilization, reliability and agility.

30. Syncsort Bigdata

Syncsort Hadoop Solutions helps on the challenges of collecting, processing and integrating data in Hadoop. It remove barriers for wider Hadoop adoption: connect, develop, deploy, re-use, and accelerate. No programming or tuning are required.

31. SGI Bigdata

SGI InfiniteData Cluster s the compute platform for Hadoop Solutions with cluster installations now reaching tens of thousands of nodes.SGI UV s the solution with the industry’s most powerful shared memory platform to find hidden data relationships or perform real-time analysis.

32. MongoDB

MongoDB is the leading NoSQL database, empowering businesses to be more agile and scalable. Fortune 500 companies and startups alike are using MongoDB to create new types of applications, improve customer experience, accelerate time to market and reduce costs.

33. Guavus Bigdata

The Guavus Reflex platform is capable of creating actionable information from widely distributed, high volume data streams in near real-time. Reflex uses highly optimized computational algorithms and machine learning to distill actionable insights from very large datasets.

34. Alteryx Bigdata

Alteryx Bigdata solution access, integration, and cleaning of sources of data as varied as Hadoop ( Cloudera & MapR) or NoSQL (MongoDB) and Excel or Teradata with predictive and spatial tools, combined in a simple, workflow design environment.

35. 1010data Advanced Analytics

The 1010data analytics platform  advanced, -in analytic functions such as Statistics (distribution analysis, correlation, variance),Predictive modeling and forecasting (linear and multivariate regression, logistic regression), Machine learning (clustering analysis, Markov chains for Monte Carlo simulations, principal component analysis). These functions are integrated directly into the system, so they run incredibly quickly on large volumes of data

36. Actian Analytics Platform

Actian Analytics Platform deliver the next generation analytics in three editions- Extreme Performance Edition, Hadoop SQL Edition, Cloud Edition.Extreme Performance Edition accelerates the analytics value chain from connecting to massive amounts of raw big data all the way to delivering actionable business value from sophisticated analytics. Hadoop SQL Edition accelerates Hadoop and makes it enterprise-grade by providing high-performance data enrichment, visual design and SQL analytics on Hadoop without the need for MapReduce skills. Cloud Edition integrates cloud and on-premises applications while providing robust data quality and other data services.

37. MapR

The MapR Distribution for Apache Hadoop provides organizations with an enterprise grade distributed data platform to reliably store and process big data. MapR packages a broad set of Apache open source ecosystem projects enabling batch, interactive, or real time applications.

38. Tableau Software bigdata

Tableau Software bigdata solutions connect to any data, anytime and anywhere, regardless of its size and complexity or mix of unstructured and structured data with the technologies like Google BigQuery and a variety of Hadoop flavors.

39. QlikView Bigdata

QlikView s two approaches to handling Big Data, both deliver the same great user experience. Either with QlikView’s 100% In-Memory Architecture or QlikView Direct , which is a hybrid approach that leverages both in-memory data and data that is dynamically queried from an external source.

40. Attivio’s Bigdata

Attivio’s Active Intelligence Engine combines Big Data and Big Content,  Hadoop. Universal indexing and automatic ad hoc JOIN of all information matching a given query, without costly data modeling and with full security. There is also Advanced text analytics that adds context and signals from human-generated information sources and support for business intelligence/data visualization tools .

41. DataStax Bigdata

DataStax Enterprise (DSE), which is  on Apache Cassandra, delivers what Internet Enterprises need to compete in today. With in-memory computing capabilities, enterprise-level security,  and powerful integrated analytics and enterprise search, visual management, and expert support, DataStax Enterprise is the leading distributed database choice for online applications that require  performance with no downtime.

42. Gooddata

The GoodData Platform is a portfolio of tools, APIs and frameworks, which makes the key components of a BI solution to collect, store, combine, analyze, and visualize. These were  to exist in the cloud and be delivered as an end-to-end service.

GoodData

GoodData
43. Google Bigdata

Google Cloud Platform surfaces the same analytical engines invented and used by Google for nearly two decades to help unearth insight in your business and operational environment. Google Cloud Platform leads the industry in the ability to let you analyze data at the scale of the entire web, with the familiarity of SQL and in a fully managed, serverless architecture where backend infrastructure is fully handled on your behalf.The big data analytics products are able to scale automatically while you focus only on the business insight you want to uncover.

44. Datameer

Datameer Professional, is a SaaS big data analytics platform targeted for department specific deployments. Datameer ing features leading Hadoop cloud providers Altiscale and Bigstep. Datameer simplifies the big data analytics environment into a single application on top of the powerful Hadoop platform.

45. CSC Big Data Platform

CSC Big Data Platform as a Service (BDPaaS) helps enterprises leap past these hurdles and get value from their data much more quickly. With BDPaaS, enterprises can rapidly develop, secure and deploy next-generation big data and analytics applications with a centralized, subscription-based platform that uses leading analytics tools, infrastructure and software.

46. Flytxt Big Data Analytics platform

Flytxt’s Big Data Analytics platform is  to integrate  Data, Big Data for deriving deeper actionable insights. It follows a hybrid architecture combining scale out clusters running Hadoop with traditional RDBMS as a metadata store and an in-memory database for Real time transactional data processing.

47. Amdocs Insight Big Data Analytics Platform

Amdocs Insight Big Data Analytics Platform supports a wide variety of Amdocs analytical applications and data services to facilitate new revenues, drive business efficiency and enhance the customer experience.

48. Cisco Bigdata

Cisco provide integrated infrastructures and analytics to support our big data partner ecosystem. Cisco UCS Integrated Infrastructure for Big Data architecture provides a secure and scalable infrastructure. Cisco is bringing the computing and analytics to the data to take advantage of the valuable insight that it reveals.

49. Platfora

 on Hadoop, Spark and native cloud APIs, Platfora’s technology helps it fit in just about anywhere  your existing analytics ecosystem, hardware and BI tools.

50. GE Bigdata

The Industrial Internet co ordinate multiple industrial applications to work intelligently in order to optimize entire operational environments.




big-data-analytics

Data analysis is nothing new. Even before computers were used, information gained in the course of business or other activities was reviewed with the aim of making those processes more efficient and more profitable. These were, of course, comparatively small-scale undertakings given the limitations posed by resources and manpower; analysis had to be manual and was slow by modern standards, but it was still worthwhile. Opinion polling, for example, has been carried out since early in the 19th century, almost 200 years ago. The first national survey took place in 1916 and involved the publication Literary Digest sending out millions of postcards and counting the returns. As a result, they correctly predicted Woodrow Wilson’s election as president.

Since then, volumes of data have grown exponentially. The advent of the internet and er computing has meant that huge quantities of information can now be harvested and used to optimise business processes. The problem is that conventional methods were simply not suited to crunching through all the numbers and making sense of them. The amount of information is phenomenal, and within that information lies insights that can be extremely beneficial. Once patterns are identified, they can be used to adjust business practices, create targeted campaigns and discard ones that are not effective. However, as well as large amounts of storage, it takes specialised software to be able to make sense of all this data in a useful way.

‘Big Data’ is the emerging discipline of capturing, storing, processing, analysing and visualising these huge quantities of information. The data sets may start at a few terabytes and run to many petabytes – far more than traditional data analysis packages can handle. In 2012 Gartner defined it as, ‘high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight  and process optimization.’ This ‘3V’ classification has been  on since (particularly with the addition of veracity), such that Big Data is often described in terms of the following characteristics:

        Volume. Terabytes or petabytes of data are analysed. An estimated 2.5 quintillion bytes of data (2.5 trillion gigabytes) are created every day, an amount which will only rise in the future. However, the size of the dataset is not the only variable that characterises Big Data.
        Variety. The dataset may contain many different forms of data – not simply a large amount of the same type. The profusion of different kinds of mobile device and the variety of content consumed on them on a wide range of platforms, for example, means that companies can harvest data from an enormous array of sources, each telling them a different part of the same picture.
        Velocity. Data may change on a constant basis. For example, modern cars may have 100 or so different sensors that continually monitor different aspects of performance. Markets change on a moment-to-moment scale. Data is highly fluid, and snapshots are not always enough.
        Veracity. The data acquired may not all be accurate, or much of it may be uncertain or provisional in nature. Data quality is unreliable, especially when there is so much of it. Any system of analysis must take this into account.

In addition to the 4V characteristics, there are also two others to deal with:

    Variability. Data capture and volume may be inconsistent, not just inaccurate, so varying quantities and qualities of data will be acquired at different times.
    Together, these factors mean that managing the data can be an extremely complex process, since there are many data sources with differing types and formats of data, but these need to be correlated and made sense of if they are to be useful.

big-data-will-drive-the-next-phase-of-innovation-in-mobile-computing

Big Data companies

Due to the nature of Big Data, specialist companies have grown up around it in order to manage the volumes and complexity of information involved.

ibm-bigdata-mobile-header

IBM Big Data Analytics

Like many other big data companies, IBM builds its ings on Hadoop – so it’s , affordable and open source. It allows businesses to capture, manage and analyse structured and unstructured data with its BigInsights product. This is also available on the cloud (BigInsights on Cloud) to give the benefits of outsourcing storage and processing, providing Hadoop as a service. InfoSphere Streams is  to enable capture and analysis of data in realtime for Internet-of-Things applications. IBM’s analytics enable powerful collating and visualisation of data with excellent flexibility for storage and management. You can also find plenty of downloadable documentation and white papers on their site.

hp4750-540x334

HP Big Data

Another well-known name in IT, HP brings a wealth of experience to big data. As well as ing their own platform, they run workshops to assess organisations’ needs. Then, ‘when you’re ready to transform your infrastructure, HP can help you develop an IT architecture that provides the capacity to manage the volume, velocity, variety, voracity, and value of your data.’ The platform itself is based on Hadoop. HP look to add value beyond providing the software alone, and will consult with you to help you craft a strategy to help you make the most of the big data you collect – and how to go about it most efficiently.

bdpmicrobd

Microsoft

Microsoft’s big data solutions run on Hadoop and can be used either in the cloud or natively on Windows. Business users can use Hadoop to gain insights into their data using standard tools  Excel or Office 365. It can be integrated with core databases to analyse both structured and unstructured data and create sophisticated 3D visualisations. Polybase is incorporated so users can then easily query and combine relational and non-relational data with the same techniques required for SQL Server. Microsoft’s solution enables you to analyse Hadoop data from within Excel, adding new functionality to a familiar software package.

30680129.cms

Intel Big Data

Recognising that making the most of big data means changing your information architecture, Intel takes the approach of enabling enterprise to create a more flexible, open and distributed environment, whilst their big data platform is based on Apache’s Hadoop. They take a thorough approach that does not assume they know what your needs are, but presents a walkthrough to determine how best to help achieve your objectives. Intel’s own industry-standard hardware is at your disposal to optimise the performance of your big data project, ing speed, scalability and a cost-effective approach according to your organisation’s requirements.

amazonwebservices-100014921-orig

Amazon Web Services

Amazon is a huge name in providing web hosting and other services, and the benefits of using them are unparalleled economies of scale and uptime. Amazon tend to  a basic framework for customers to use, without providing much in the way of customer support. This means they are the ideal choice if you know exactly what you are doing and want to save money. Amazon supports products like Hadoop, Pig, Hive and Spark, enabling you to build your own solution on their platform and create your own big data stack. There are plenty of tutorials, video demos and guides to get you started as quickly and easily as possible.

dell-software-logo

Dell Big Data Analytics

Another well known and globally-established company, this time in the hardware space, Dell s its own big data package. Their solution  an automated facility to load and continuously replicate changes from an Oracle database to a Hadoop cluster to support big data analytics projects, thereby simplifying Oracle and Hadoop data integration. Data can be integrated in near real-time, from a wide range of data stores and applications, and from both on- and off-premises sources. Techniques such as natural language processing, machine learning and sentiment analysis are made accessible through straightforward search and powerful visualisation to enable users to learn relationships between different data streams and leverage these for their businesses.

Teradata-Logo-620x265

Teradata

Teradata call their big data product a ‘data warehouse system’, which stores and manages data. The different server nodes share nothing, having their own memory and processing power, and each new node increases storage capacity. The database sits over these and the workload is shared among them. The company started taking an interest in big data in 2010, adding analytics for text documents,  unstructured data and semi-structured data (e.g. word processor documents and spreadsheets). They also work with unstructured data gathered from online interactions.

bigquery_0

Google BigQuery

Google is the big daddy of internet search: the outright market leader with the vast majority of search traffic to its name. No other search engine comes close, so perhaps it’s not surprising that Google should  an analytics package to crunch through the phenomenal amount of data it produces in the course of its day-to-day work for millions of businesses around the world. It already hosts the hugely popular Google Analytics, but BigQuery is  for a different order of magnitude of data. It puts Google’s impressive infrastructure at your disposal, allowing you to analyse massive datasets in the cloud with , SQL-like queries – analysing multi-terabyte datasets in just seconds. Being Google it’s also very scalable and straightforward to use.

vmware-logo

VMware Big Data

VMware is well-known in the world of best cloud storage and IaaS. Their big data solutions use their established vSphere product to virtualise Hadoop whilst maintaining excellent performance.  and elastic scaling is possible due to an approach that separates out storage from computing, keeping data safe and persistent, enabling greater efficiency and flexibility. Essentially this is a sophisticated and safe approach to Hadoop-as-a-service, which utilises many of VMware’s strengths to deliver a big data platform reliably and in a cost-effective way.

red-hat-1-large

Redhat

As might be expected, Redhat take an open source approach to big data, believing that changing workloads and technologies require an open approach. They take a modular approach so that the building blocks of their platform work interoperably with other elements of your data centre. Building blocks include Platform-as-a-Service (PaaS), so you can develop apps er, process data in real time, and easily integrate systems; Infrastructure-as-a-Service (IaaS), to enable deployment and management of service providers, tools, and components of IT architecture across platforms and technology stacks in a consistent, unified way; Middleware, integration and automation, to streamline data sources and interaction; and Storage, of the most appropriate kind for the task in hand.

it_photo_102724

Tableau Software

Tableau s significant flexibility over how you work with data. Using Tableau’s own servers and Desktop visualisation with your existing big data storage makes it a versatile and powerful system. There are two options: connecting to your data live, or bringing it into memory for  response queries. Memory management means all laptop/PC memory is used, down to the hard disk, to maintain speed and performance, even at large scale. Tableau supports more than 30 databases and formats, and is easy to connect to and manage. Multi-million row tables can be visually analysed directly on the database itself, extremely quickly.

informatica-cloud

Informatica Big Data

Another provider that builds its platform on Hadoop, Informatica has several options that make life easy by giving you access to the functionality and allow you to integrate all types of data efficiently without having to learn Hadoop itself. Informatica Big Data Edition uses a visual development environment to save time and improve accessibility (Informatica claims this makes it approximately five times er than hand-coding a solution). This also has the advantage of not needing to hire dedicated Hadoop experts, since there are more than 100,000 Informatica experts worldwide. This makes for a fantastically versatile solution that is still simple enough to be used without intensive training.

splunk-logo

Splunk

Splunk collects and analyses machine data as it comes in. Realtime alerts are used to spot trends and identify patterns as they occur. It’s extremely easy to deploy and use, and highly scalable: ‘from a single server to multiple datacenters.’ There is also a strong emphasis on security, with role-based access controls and auditability. Splunk is  for Hadoop and NoSQL data stores to enable analysis and visualisation of unstructured data. There’s also a community forum and online support centre, should you need assistance getting set up or figuring out how things work.

datastax_logo_blue

DataStax Big Data

DataStax big data solution is  on Apache Cassandra, an open source and enterprise-ready platform that is commercially supported. It is used by a number of the world’s most innovative and best-known companies, such as Netflix and eBay. Their chief product, DataStax Enterprise, leverages Cassandra’s properties to give vast scalability, continuous availability and strong security. The combination of commercial software and open source platform means that it’s  and low-cost compared to many other options on the market. It’s also relatively easy to run. DataStax boast that their product ‘enables you to perform real-time transactions with Cassandra, analytics with Apache Hadoop and enterprise search with Apache Solr, in a single, smartly integrated big data platform that works across multiple datacenters and the cloud.

MongoDB_Logo_Full

MongoDB

‘Mongo’ comes from ‘humongous’ and takes a different approach to normal, using JSON-like documents instead of table-based relational database structures. This allows it to integrate certain types of data er and more easily. Is it free and open-source software, released under a combination of the GNU Affero General Public License and the Apache License. Mongo has been adopted by a number of well-known and very large websites, such as Craigslist, eBay and the New York Times. Mongo’s analytics are  to scale and are  into the operational database, meaning you have access to them in realtime.

gooddata_vertical_black-1

Gooddata

Gooddata is an all-in-one cloud analytics platform. They have a wide range of customers,  HP and Nestle. Operating fully in the cloud, Gooddata manage hosting, data and technology, meaning that the customer is able to focus completely on the analytics. They are recognised as industry leaders, with a number of awards to their name,  from Gartner. There’s an emphasis on usability, with interactive dashboards that facilitate collaboration by team-members as well as visual data , so that teams can move quickly on insights gained. The responsive UI is  to be easy to use on any device or platform,  mobile devices.

qlikview_logo_large

QlikView

QlikView s two big data solutions, enabling users to switch between them as the require. Their In-Memory architecture uses a patented data engine to compress data by a factor of 10, so that up to 2 TB can be stored on a 256 GB RAM server. This s exceptional performance, and other features further enhance response rates and make exploring very large data sets extremely . This is used by many of Qlik’s customers to analyse volumes of data stored in data warehouses or Hadoop clusters. This hybrid approach means big data can be made accessible to users without knowledge of programming. It also allows a highly focused and granular view of data when required.

Attivio-Small-PR-trim

Attivio

Attivio’s Active Intelligence Engine (AIE) brings together a number of separate capabilities – business intelligence, enterprise search, business analytics, data warehousing and process automation – to produce comprehensive information, presented in a user-friendly way. AIE puts together both structured and unstructured data into one index to be searched, collated and analysed; regular search queries and SQL can be used and a wide range of queries are therefore possible, from broad to highly focused. It can be integrated with a large number of data sources by giving it access with other software applications. It uses proprietary, patented technology, unlike many of its open-source-based rivals.

img15

1010data Advanced Analytics

1010data s a complete suite of products, enabling companies to engage with the data they harvest in their everyday business. Data is analysed on the same platform on which it is stored, minimising delays from moving data. This enables  responses to changing market information and an agile approach that reacts in near-realtime. There is ‘immediate, direct, unfettered access to all relevant data, even voluminous, granular, raw data’. 1010’s platform can be implemented on the cloud, so that anyone with the correct access rights can use it from anywhere in the world. The company s an ‘Analytical Platform as a Service’ (APaaS) approach that gives enterprise-grade cloud security, reliability, and interoperability, along with cost-effective, on-demand performance and storage scalability.

Actian-logo

Actian

Actian’s Vortex is  on Apache Hadoop, an open source framework written in Java for distributed storage and processing of very large data sets. This means that Actian’s big data solutions will always be open themselves, so that customers are not locked into a proprietary platform. They claim their software is , despite the large size of the datasets they deal with. Whilst Hadoop is complex, Actian’s platform is far more straightforward to use, making it enterprise ready and emphasising security and scalability. It gives full SQL support to your data. Actian is used by thousands of big-name customers worldwide,  Nikon, China Telecom and GE Transportation.

Conclusion

Big data isn’t just an emerging phenomenon. It’s already here and being used by major companies to drive their business forwards. Traditional analytics packages simply aren’t capable of dealing with the quantity, variety and changeability of data that can now be harvested from diverse sources – machine sensors, text documents, structured and unstructured data, social media and more. When these are combined and analysed as a whole, new patterns emerge. The right big data package will allow enterprises to track these trends in real time, spotting them as they occur and enabling businesses to leverage the insights provided.

However, not all big data platforms and software are alike. As ever, which you decide on will depend on a number of factors. These include not just the nature of the data you are working with, but organisational budgets, infrastructure and the skillset of your team, amongst other things. Some solutions are  to be used off-the-peg, providing powerful visualisations and connecting easily to your data stores. Others are intended to be more flexible but should only be used by those with coding expertise. You should also think to the future, and the long-term implications of being tied to your platform of choice – particularly in terms of open-source vs proprietary software.


Big data refers to massive,
heterogeneous, and often
unstructured digital content that is
difficult to process using traditional
data management tools and
techniques. The term encompasses
Published by the IEEE Computer Society	
the complexity and variety of data
and data types, real-time data
collection and processing needs, and
the value that can be obtained by
smart analytics.
Advanced data mining techniques
and associated tools can help
extract information from large,
complex datasets that is useful
in making informed decisions
in many business and scientific
applications  tax payment
collection, market sales, social
studies, biosciences, and high-
energy physics. Combining big data
analytics and knowledge 
techniques with scalable computing
systems will produce new insights in
a shorter time.
Although few cloud-based
analytics platforms are available
today, current research work
anticipates that they will become
common within a few years. Some
current solutions are based on open
0018-9162/13/$31.00 © 2013 IEEEsource systems such as Apache
Hadoop and SciDB, while others
are proprietary solutions provided
by companies such as Google, IBM,
EMC, BigML, Splunk Storm, Kognitio,
and InsightsOne.
As more such platforms emerge,
researchers will port increasingly
powerful data mining programming
tools and strategies to the cloud
to exploit complex and flexible
software models such as the
distributed workflow paradigm.
The growing use of service-oriented
computing could accelerate this
trend (http://tinyurl.com/d26o2j5).
DATA ANALYTICS SERVICE
MODELS
Developers and researchers can
adopt the software as a service
(SaaS), platform as a service (PaaS),
and infrastructure as a service
(IaaS) models to implement big data
analytics solutions in the cloud.
The SaaS model s complete
big data analytics applications
to end users, who can exploit
the cloud’s scalability in both
data storage and processing
power to execute analysis on
large or complex datasets.
The PaaS model provides data
analytics programming suites and
environments in which data mining
developers can design scalable
analytics services and applications.
Researchers can exploit the
IaaS model to compose a set of
virtualized hardware and software
resources for running data analysis
frameworks or applications.
Column Contributions
W
e welcome short articles (1,500 to
2,000 words) for publication in the
Cloud Cover column that  the
questions outlined in Computer’s
January 2013 issue (S. Murugesan,
As Table 1 shows, developers
can implement big data analytics
services within each of these three
models:
•	 data analytics software as a ser-
vice—provides a well-defined
data mining algorithm or ready-
to-use knowledge  tool
as an Internet service to end
users, who can access it directly
through a Web browser;
•	 data analytics platform as a ser-
vice—provides a supporting
platform that developers can
use to build their own data ana-
lytics applications or extend
existing ones without concern
about the underlying infrastruc-
ture or distributed computing
issues; and
•	 data analytics infrastructure
as a service—provides a set of
virtualized resources that devel-
opers can use as a computing
infrastructure to run data
mining applications or to imple-
ment data analytics systems
from scratch.
End users whose goal is to per-
form complex data analysis can
“Cloud Computing: The New Normal?,”
pp. 77-79). Submit your ideas for
advancing the technology or share your
experiences in harnessing the cloud at
cloudcover@computer.org.
apply the recently implemented Data
Mining Cloud Framework (http://
tinyurl.com/c4b4f5k) as a high-level
PaaS programming environment
and create a set of SaaS suites for big
data analytics. With this approach,
users need not be concerned about
cloud platform or application pro-
gramming details.
BIG DATA ANALYTICS
WORKFLOWS
Developers can use workflows,
which consist of complex graphs
of many concurrent tasks, to
 the complexity of scientific
and business applications. This
approach supports data analytics
design by providing a paradigm that
encompasses all the steps of data
analytics, from data access and
filtering to data mining and sharing
produced knowledge.
Workflow-based data mining
frameworks that run on cloud
platforms and use a service-
oriented approach  a flexible
programming model, distributed
task interoperability, and
execution scalability that reduces
data analytics completion time.
Application developers can design
Table 1. Cloud-based data analytics services.
Cloud service model
Features
Users
Data analytics software as a service A single and complete data mining application or task
( data sources) ed as a service End users, analytics managers, data
analysts
Data analytics platform as a service A data analysis suite or framework for programming or
developing high-level applications, hiding the cloud infra-
structure and data storage Data mining application developers,
data scientists
Data analytics infrastructure as a
service A set of virtualized resources provided to a programmer or
data mining researcher for developing, configuring, and
running data analysis frameworks or applications Data mining programmers, data
management developers, data
mining researchers
	
MAY 2013	
99Clo ud C ov er
Figure 1. Data analysis workflow application  using the Data Mining Cloud Framework’s graphical programming interface.
data analysis tasks, scientific
computation methods, and complex
simulation techniques as workflows
that integrate single Web services
and execute them concurrently on
virtual machines in the cloud.
Figure 1 shows a data analysis
workflow application  using
the Data Mining Cloud Framework’s
graphical programming interface
recently developed in our laboratory
(http://tinyurl.com/crnork2). Data
sources and tools such as data
mining algorithms, filters, and data
splitters are connected through
direct edges that define specific
dependency relationships among
them.
When creating an edge between
two nodes, the system automatically
attaches a label to it that represents
the relationship between them.
To ease workflow composition
and allow users to monitor its
execution, each resource icon has
an associated tag—the checkmarks
in Figure 1—representing the status
of a corresponding resource.
The experimental results of a
set of studies using the framework
to analyze genomics, network
intrusion, and bioinformatics data
demonstrated its effectiveness,
as well as the linear scalability
achieved through concurrent
execution of the workflow tasks on a
pool of virtual servers (http://tinyurl.
com/c4b4f5k).
Current research focuses on the
workflow composition interface,
with the aim of extending supported
design patterns such as conditional
branches and iterations and
evaluating its functionality and
Cloud Computing
Special Technical Community
T
he CS Cloud Computing Special
Technical Community (CS CCSTC)
focuses on cloud activities across
the IEEE Computer Society, involving
both CS members and nonmembers.
The STC’s work is complementary
	100	
computer
to the IEEE Cloud Computing
Initiative (IEEE CCI), a three-year
project to promote cloud efforts
across IEEE.
For details or to join the STC, visit
www.computer.org/cc.
performance during the design and
execution of complex data mining
workflows on large datasets in the
cloud.
RESEARCH
RECOMMENDATIONS
Cloud-based data analytics
requires high-level, easy-to-use
design tools for programming
large applications dealing with
huge, distributed data sources.
This necessitates further research
and development in several key
areas.
•	 Programming abstracts for big
data analytics. Big data analyt-
ics programming tools require
novel complex abstract struc-
tures. The MapReduce model
is often used on clusters and
clouds, but more research is
needed to develop scalable
higher-level models and tools.
•	 Data and tool interoperability
and openness. Interoperability
is a main issue in large-scale
applications that use resources
such as data and computing
nodes. Standard formats and
models are needed to support
interoperability and ease co-
operation among teams usingdifferent data formats and
tools.
•	 Integration of big data analytics
frameworks. The service-
oriented paradigm allows run-
ning large-scale distributed
workflows on heterogeneous
platforms along with software
components developed using
different programming lan-
guages or tools. The Web and
cloud services paradigms can
help manage worldwide integra-
tion of multiple data analytics
frameworks.
•	 Data provenance and annota-
tion mechanisms. Provenance
is captured as a set of depen-
dencies between elements that
researchers can use to interpret
data and provide reproducible
analysis. Research is needed to
develop innovative techniques
for visualizing and mining prov-
enance data.
These solutions, together with
others ing data privacy and
security concerns, will promote
cloud-based data analytics in large
companies, and eventually will
benefit users such as independent
research teams, start-ups, and
small enterprises that aren’t deeply
skilled in cloud programming and
management.
A
dvancing the cloud
from a computation
and data management
infrastructure to a pervasive and
scalable data analytics platform


